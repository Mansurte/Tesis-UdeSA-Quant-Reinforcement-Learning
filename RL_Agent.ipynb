{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNPIJPExfrY4"
   },
   "source": [
    "# Alonso, Ariel - MFE Thesis - Reinforcement Learning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data configuration definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q0btfH4WZhK"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/Ariel/Documents/Maestria/TESIS/Mi Tesis/'\n",
    "\n",
    "# Definition for data dates to use\n",
    "DATA_DATES = ['18-12-19', '19-12-19', '20-12-19', '06-01-20', '07-01-20']\n",
    "\n",
    "# Definition of order book data for the agents\n",
    "TOP_OF_BOOK = ['Bid1_Price', 'Bid1_Size',\n",
    "               'Ask1_Price', 'Ask1_Size']\n",
    "FULL_BOOK = ['Bid1_Price', 'Bid1_Size',\n",
    "             'Bid2_Price', 'Bid2_Size',\n",
    "             'Bid3_Price', 'Bid3_Size',\n",
    "             'Bid4_Price', 'Bid4_Size',\n",
    "             'Bid5_Price', 'Bid5_Size',\n",
    "             'Ask1_Price', 'Ask1_Size',\n",
    "             'Ask2_Price', 'Ask2_Size',\n",
    "             'Ask3_Price', 'Ask3_Size',\n",
    "             'Ask4_Price', 'Ask4_Size',\n",
    "             'Ask5_Price', 'Ask5_Size',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CHARACTERISTICS\n",
    "\n",
    "MARKET_IMBALANCE = True # Defines if we summarize the entire book into one number\n",
    "SIMPLIFY_INDICATORS = True # Defines if simplify the MACD signal and RSI indicators\n",
    "BID_ASK_SPREAD = True # Defines if we add the bid_ask_spread column to the dataframe\n",
    "QUANTIZE = False # Defines if we quantize the size_imbalance and the bid_ask_spread columns\n",
    "REWARD_HORIZON = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd8pz6d4OL6x"
   },
   "outputs": [],
   "source": [
    "# Load dataframes in lists\n",
    "df_data_list = []\n",
    "for date in DATA_DATES:\n",
    "    df_data_list.append(pd.read_csv('{}AY24D_{}.csv'.format(DATA_PATH, date)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut first and last entries of the dataframes to remove inaccurate data\n",
    "ENTRIES_REMOVED = 1000\n",
    "ENTRIES_RETAINED = 1000\n",
    "START = 3000\n",
    "\n",
    "for i, df in enumerate (df_data_list):\n",
    "    df = df[ENTRIES_REMOVED:-ENTRIES_REMOVED]\n",
    "    if i < 5:\n",
    "        df = df[START:START + ENTRIES_RETAINED]\n",
    "    df_data_list[i] = df.reset_index(drop = True)\n",
    "    df_data_list[i].drop(columns=['Datetime'], inplace = True) # We drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical indicators for enriching the dataset\n",
    "\n",
    "import talib\n",
    "INDICATORS_EMA_SPAN_MULTIPLIER = 10 # We augment the period of the MACD EMAs to filter short term noise better\n",
    "RSI_THRESHOLD = 40 # Defines oversold and overbought percentages, ranges from 0 to 50\n",
    "\n",
    "def MACD(df):\n",
    "    '''Calculates the MACD and MACD SIGNAL indicators'''\n",
    "    EMA_12 = df.Mid_Price.ewm(span=12*INDICATORS_EMA_SPAN_MULTIPLIER, adjust=False).mean()\n",
    "    EMA_26 = df.Mid_Price.ewm(span=26*INDICATORS_EMA_SPAN_MULTIPLIER, adjust=False).mean()\n",
    "    MACD = EMA_12 - EMA_26\n",
    "    MACD_SIGNAL = MACD.ewm(span=9, adjust=False).mean()\n",
    "    return MACD, MACD_SIGNAL\n",
    "\n",
    "def RSI(df):\n",
    "    '''Calculates the RSI indicator'''\n",
    "    RSI = talib.RSI(df['Mid_Price'], timeperiod = 14*INDICATORS_EMA_SPAN_MULTIPLIER)\n",
    "    return RSI\n",
    "\n",
    "def indicators_enrichment(df):\n",
    "    '''Incorporates indicators to the dataframe'''\n",
    "    df['Mid_Price'] = (df['Bid1_Price'] + df['Ask1_Price'])/2\n",
    "    \n",
    "    # MACD & SIGNAL\n",
    "    df['MACD'], df['MACD_SIGNAL'] = MACD(df)\n",
    "    if SIMPLIFY_INDICATORS:\n",
    "        # Simplifying the indicator\n",
    "        df['MACD_SIGNAL_CROSS'] = np.sign(np.sign(df['MACD'] - df['MACD_SIGNAL']).diff())\n",
    "        df.fillna({'MACD_SIGNAL_CROSS':0}, inplace=True)\n",
    "        df.drop(columns=['MACD', 'MACD_SIGNAL'], inplace=True)\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = RSI(df)\n",
    "    df.fillna({'RSI':50}, inplace=True)# We fill the NaN with an indicator neutral value\n",
    "    if SIMPLIFY_INDICATORS:\n",
    "        # Simplifying the indicator\n",
    "        overbought =  df['RSI'] > (100 - RSI_THRESHOLD)\n",
    "        oversold   =  df['RSI'] < RSI_THRESHOLD\n",
    "        neither    = (df['RSI'] > RSI_THRESHOLD) & (df['RSI'] < (100 - RSI_THRESHOLD))\n",
    "        df['RSI'][overbought] = -1\n",
    "        df['RSI'][oversold]   =  1\n",
    "        df['RSI'][neither]    =  0\n",
    "    \n",
    "    df.drop(columns=['Mid_Price'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# We apply the new indicators and incorporate them into the dataframes\n",
    "for i, df in enumerate (df_data_list):\n",
    "    df_data_list[i] = indicators_enrichment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bid-Ask spread feature\n",
    "def bid_ask_spread(df):\n",
    "    df['Bid_Ask_spread'] = df['Ask1_Price'] - df['Bid1_Price'] \n",
    "    return df\n",
    "\n",
    "# We apply the new indicators and incorporate them into the dataframes\n",
    "if BID_ASK_SPREAD:\n",
    "    for i, df in enumerate (df_data_list):\n",
    "        df_data_list[i] = bid_ask_spread(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4I3_Ruph3Xl"
   },
   "outputs": [],
   "source": [
    "# Defining agents, discarding irrelevant data for TOB agent\n",
    "df_FB = df_data_list\n",
    "df_TOB = []\n",
    "for i, df in enumerate (df_data_list):\n",
    "    df_TOB.append(df.drop(columns=[e for e in FULL_BOOK if e not in TOP_OF_BOOK]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Imbalance calculations\n",
    "tau = 1/20 # Penalizes bids and offers at distant prices\n",
    "\n",
    "def book_imbalance(df, FULL_BOOK_AGENT):\n",
    "    if FULL_BOOK_AGENT:\n",
    "        Bid2_equiv = np.exp( - (df['Bid1_Price'] - df['Bid2_Price']) / tau ) * df['Bid2_Size']\n",
    "        Bid3_equiv = np.exp( - (df['Bid1_Price'] - df['Bid3_Price']) / tau ) * df['Bid3_Size']\n",
    "        Bid4_equiv = np.exp( - (df['Bid1_Price'] - df['Bid4_Price']) / tau ) * df['Bid4_Size']\n",
    "        Bid5_equiv = np.exp( - (df['Bid1_Price'] - df['Bid5_Price']) / tau ) * df['Bid5_Size']\n",
    "        Ask2_equiv = np.exp( - (df['Ask2_Price'] - df['Ask1_Price']) / tau ) * df['Ask2_Size']\n",
    "        Ask3_equiv = np.exp( - (df['Ask3_Price'] - df['Ask1_Price']) / tau ) * df['Ask3_Size']\n",
    "        Ask4_equiv = np.exp( - (df['Ask4_Price'] - df['Ask1_Price']) / tau ) * df['Ask4_Size']\n",
    "        Ask5_equiv = np.exp( - (df['Ask5_Price'] - df['Ask1_Price']) / tau ) * df['Ask5_Size']\n",
    "    \n",
    "        df['Size_Imbalance'] = (df['Bid1_Size'] - df['Ask1_Size'])  + \\\n",
    "                                 (Bid2_equiv - Ask2_equiv)    + \\\n",
    "                                 (Bid3_equiv - Ask3_equiv)    + \\\n",
    "                                 (Bid4_equiv - Ask4_equiv)    + \\\n",
    "                                 (Bid5_equiv - Ask5_equiv)\n",
    "    \n",
    "        DROP_COLUMNS = [e for e in FULL_BOOK if e not in ('Bid1_Price','Ask1_Price')]\n",
    "        df.drop(columns=DROP_COLUMNS, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        df['Size_Imbalance'] = df['Bid1_Size'] - df['Ask1_Size']\n",
    "        df.drop(columns=['Bid1_Size', 'Ask1_Size'], inplace=True)\n",
    "        return df    \n",
    "\n",
    "if MARKET_IMBALANCE:\n",
    "    for i, df in enumerate (df_TOB):\n",
    "        df_TOB[i] = book_imbalance(df, False)\n",
    "    for i, df in enumerate (df_FB):\n",
    "        df_FB[i] = book_imbalance(df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series quantizing\n",
    "def quantize(df):\n",
    "    QUANTILES_QTY = 4\n",
    "    df['Size_Imbalance'] = pd.qcut(df['Size_Imbalance'], QUANTILES_QTY, labels=False, duplicates = 'drop')\n",
    "    df['Bid_Ask_spread']   = pd.qcut(df['Bid_Ask_spread'], QUANTILES_QTY, labels=False, duplicates = 'drop')\n",
    "    return df\n",
    "\n",
    "# We apply the new quantizing method to the dataframes\n",
    "if QUANTIZE:\n",
    "    for i, df in enumerate (df_TOB):\n",
    "        df_TOB[i] = quantize(df)\n",
    "    for i, df in enumerate (df_FB):\n",
    "        df_FB[i] = quantize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date selection of the training, validation and testing datasets\n",
    "TRAIN_DATA = ['18-12-19', '19-12-19', '20-12-19']\n",
    "VALIDATION_DATA  = '06-01-20'\n",
    "TEST_DATA = '07-01-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain the final dataframes of training and testing\n",
    "df_validation_TOB_agent  = df_TOB[DATA_DATES.index(VALIDATION_DATA)]\n",
    "df_validation_FB_agent   = df_FB[DATA_DATES.index(VALIDATION_DATA)]\n",
    "\n",
    "df_test_TOB_agent = df_TOB[DATA_DATES.index(TEST_DATA)]\n",
    "df_test_FB_agent   = df_FB[DATA_DATES.index(TEST_DATA)]\n",
    "\n",
    "df_TOB_aux = df_TOB.copy()\n",
    "df_FB_aux = df_FB.copy()\n",
    "\n",
    "del df_TOB_aux[DATA_DATES.index(TEST_DATA)]\n",
    "del df_TOB_aux[DATA_DATES.index(VALIDATION_DATA)]\n",
    "del df_FB_aux[DATA_DATES.index(TEST_DATA)]\n",
    "del df_FB_aux[DATA_DATES.index(VALIDATION_DATA)]\n",
    "\n",
    "# Now we keep the daily cut indexes between days to prevent day hopping in training\n",
    "daily_cuts = np.array([len(item) for item in df_TOB_aux])\n",
    "daily_cuts = np.insert(daily_cuts, 0, 0) # We insert a 0 in the first position as this indicates the first day start\n",
    "\n",
    "# We now concatenate all the traning dataframes into one, resetting the indexes, we already have the daily cuts\n",
    "df_train_TOB_agent = pd.concat(df_TOB_aux, ignore_index = True)\n",
    "df_train_FB_agent  = pd.concat(df_FB_aux, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agents Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Re5o2dOXKZYG"
   },
   "outputs": [],
   "source": [
    "# TRAINING ENVIRONMENT\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "TRAINING_DAY_COUNT = len(TRAIN_DATA)\n",
    "EPISODE_LENGTH = len(df_train_TOB_agent)\n",
    "\n",
    "POSITION_LIMITS = False\n",
    "MAX_CASH = 10000\n",
    "MAX_BOND_QTY = 5\n",
    "\n",
    "POSITION_PENALTY = False\n",
    "POSITION_PENALTY_AMOUNT = 10\n",
    "\n",
    "TRADING_PENALTY = False\n",
    "NON_TRADING_PENALTY_AMOUNT = 1\n",
    "\n",
    "POSITIVE_REWARD_BOOSTER = False\n",
    "REWARD_BOOST_FACTOR = 10\n",
    "\n",
    "REWARD_IMMEDIATE = True\n",
    "REWARD_END_OF_EPISODE = False\n",
    "\n",
    "HIGH_FREQ_PENALTY = False\n",
    "HIGH_FREQ_TICKS = REWARD_HORIZON + 1\n",
    "HIGH_FREQ_PENALTY_AMOUNT = 10\n",
    "\n",
    "\n",
    "class AY24BooksTrainEnv(gym.Env):\n",
    "    BUY  = 0\n",
    "    SELL = 1\n",
    "    HOLD = 2\n",
    "    \n",
    "    def __init__(self, df, daily_cuts = np.array([]), agent_bond_qty = 0, agent_cash = 0, tick = 0):\n",
    "        self.df = df\n",
    "        self.daily_cuts = daily_cuts\n",
    "        self.daily_cuts_cumsum = daily_cuts.cumsum().tolist()\n",
    "        self.traning_day = 0\n",
    "        self.tick = tick\n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        self.cum_reward = 0\n",
    "        self.hf_ticks = 0\n",
    "        n_actions = 3\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=[self.df.shape[1]  - 2], dtype=np.double)\n",
    "        # The observation space includes bid-ask spread, MACD signal, RSI signal and market imbalance features\n",
    "        # The - 2 results from substracting the bid and ask prices from the feature set\n",
    "        self.agent_bond_qty = agent_bond_qty\n",
    "        self.agent_cash = agent_cash\n",
    "        self.data = self.df.loc[self.tick,:]\n",
    "        self.future_prices = self.df.loc[self.tick + REWARD_HORIZON,:]\n",
    "        self.df_normalized = self.normalize_input_data()\n",
    "        self.df_normalized.drop(columns=['Bid1_Price','Ask1_Price'], inplace=True)\n",
    "        self.data_normalized = self.df_normalized.loc[self.tick,:]\n",
    "        self.state = self.data_normalized.tolist()\n",
    "        self.pending_buy  = []\n",
    "        self.pending_sell = []\n",
    "          \n",
    "    def normalize_input_data(self):\n",
    "        x = self.df.values #returns a numpy array\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        df_normalized = pd.DataFrame(x_scaled, columns=self.df.columns)\n",
    "        return df_normalized\n",
    "    \n",
    "    def reset(self):\n",
    "        self.day = 0\n",
    "        self.tick = 0\n",
    "        self.initial_tick = self.tick\n",
    "        self.agent_cash = 0\n",
    "        self.agent_bond_qty = 0\n",
    "        self.passive = True\n",
    "        self.reward = 0\n",
    "        self.cum_reward = 0\n",
    "        self.hf_ticks = 0\n",
    "        self.data = self.df.loc[self.tick,:]\n",
    "        self.future_prices = self.df.loc[self.tick + REWARD_HORIZON,:]\n",
    "        self.data_normalized = self.df_normalized.loc[self.tick,:]\n",
    "        self.state = self.data_normalized.tolist()\n",
    "        observation = np.array(self.state)\n",
    "        self.pending_buy  = []\n",
    "        self.pending_sell = []\n",
    "        return observation\n",
    "    \n",
    "    def buy_bond(self):\n",
    "        self.agent_cash -= self.data.Ask1_Price\n",
    "        self.agent_bond_qty += 1\n",
    "        if POSITION_LIMITS:\n",
    "            if self.agent_bond_qty < MAX_BOND_QTY and self.agent_cash > -MAX_CASH:\n",
    "                self.agent_cash -= self.data.Ask1_Price\n",
    "                self.agent_bond_qty += 1\n",
    "            else:\n",
    "                print(\"Can't buy, maximum bond quantity reached or maximum negative cash limit reached\")\n",
    "\n",
    "    def sell_bond(self):\n",
    "        self.agent_cash += self.data.Bid1_Price\n",
    "        self.agent_bond_qty -= 1\n",
    "        if POSITION_LIMITS:\n",
    "            if self.agent_bond_qty >= -MAX_BOND_QTY and self.agent_cash < MAX_CASH:\n",
    "                self.agent_cash += self.data.Bid1_Price\n",
    "                self.agent_bond_qty -= 1\n",
    "            else:\n",
    "                print(\"Can't sell, maximum negative bond quantity reached or maximum cash limit reached\")\n",
    "        \n",
    "    def step(self, action):\n",
    "        if action == self.BUY:\n",
    "            print('Action: Buy')\n",
    "            self.buy_bond()\n",
    "            self.pending_sell.append(self.tick + REWARD_HORIZON)\n",
    "            print('I am buying at tick', self.tick, ' and will sell at tick', self.tick + REWARD_HORIZON)\n",
    "            self.passive = False\n",
    "            if HIGH_FREQ_PENALTY and self.hf_ticks < HIGH_FREQ_TICKS:\n",
    "                self.reward =- HIGH_FREQ_PENALTY_AMOUNT\n",
    "            elif REWARD_IMMEDIATE:\n",
    "                self.reward = self.future_prices.Bid1_Price - self.data.Ask1_Price\n",
    "            else:\n",
    "                self.reward = 0\n",
    "            self.hf_ticks = 0\n",
    "            if POSITIVE_REWARD_BOOSTER:\n",
    "                if self.reward > 0:\n",
    "                    self.reward *= REWARD_BOOST_FACTOR\n",
    "        \n",
    "        elif action == self.SELL:\n",
    "            print('Action: Sell')\n",
    "            self.sell_bond()\n",
    "            self.pending_buy.append(self.tick + REWARD_HORIZON)\n",
    "            print('I am selling at tick', self.tick, ' and will buy at tick', self.tick + REWARD_HORIZON)\n",
    "            self.passive = False\n",
    "            if HIGH_FREQ_PENALTY and self.hf_ticks < HIGH_FREQ_TICKS:\n",
    "                self.reward =- HIGH_FREQ_PENALTY_AMOUNT\n",
    "            elif REWARD_IMMEDIATE:\n",
    "                self.reward = self.data.Bid1_Price - self.future_prices.Ask1_Price\n",
    "            else:\n",
    "                self.reward = 0\n",
    "            self.hf_ticks = 0\n",
    "            if POSITIVE_REWARD_BOOSTER:\n",
    "                if self.reward > 0:\n",
    "                    self.reward *= REWARD_BOOST_FACTOR\n",
    "                    print('Reward boosteada=', self.reward)\n",
    "        elif action == self.HOLD:\n",
    "            print('Action: Hold')\n",
    "            self.reward = 0\n",
    "            self.hf_ticks += 1\n",
    "        else:\n",
    "            print('Error: Invalid action')\n",
    "        \n",
    "        # We close previously opened position\n",
    "        if self.tick in self.pending_buy:\n",
    "            print('Buying to close position...')\n",
    "            self.buy_bond()\n",
    "            self.pending_buy.remove(self.tick)\n",
    "        elif self.tick in self.pending_sell:\n",
    "            print('Selling to close position...')\n",
    "            self.sell_bond()\n",
    "            self.pending_sell.remove(self.tick)\n",
    "\n",
    "        # When the end of the day approaches, we stop and close the opened position at the corresponding tick\n",
    "        if self.tick >= (self.daily_cuts[self.day+1] - REWARD_HORIZON):\n",
    "            for i in self.pending_buy:\n",
    "                self.tick = i\n",
    "                self.data = self.df.loc[self.tick,:]\n",
    "                self.buy_bond()\n",
    "            self.pending_buy = []\n",
    "            for i in self.pending_sell:\n",
    "                self.tick = i\n",
    "                self.data = self.df.loc[self.tick,:]\n",
    "                self.sell_bond()\n",
    "            self.pending_sell = []\n",
    "            \n",
    "            self.day += 1\n",
    "            self.tick = self.daily_cuts_cumsum[self.day]\n",
    "        \n",
    "        # When the end of the whole df is reached, the terminal state needs to be checked:\n",
    "        ptf_value = self.agent_bond_qty * self.data.Bid1_Price + self.agent_cash if self.agent_bond_qty > 0 \\\n",
    "                    else self.agent_bond_qty * self.data.Ask1_Price + self.agent_cash\n",
    "        self.terminal = self.tick >= self.initial_tick + EPISODE_LENGTH\n",
    "        if self.terminal:\n",
    "            print('---------END OF EPISODE----------')\n",
    "            print('self.agent_cash=', round(self.agent_cash,2))\n",
    "            print('self.agent_bond_qty=', self.agent_bond_qty)\n",
    "            print('ptf_value=', ptf_value)\n",
    "            if REWARD_END_OF_EPISODE:\n",
    "                self.reward += ptf_value  # CASO DE REWARD AL FINAL\n",
    "            if self.passive and TRADING_PENALTY:\n",
    "                self.reward -= NON_TRADING_PENALTY_AMOUNT\n",
    "                print('Test reward after NTP =', self.reward)\n",
    "            if POSITIVE_REWARD_BOOSTER:\n",
    "                if self.reward > 0:\n",
    "                    self.reward *= REWARD_BOOST_FACTOR # NO BOOST AT THE END\n",
    "                    print('reward =',self.reward)\n",
    "            print('---------------------------------')\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        \n",
    "        # If no terminal state is reached, the training continues\n",
    "        \n",
    "        # We return the info of the next tick and the cash and position state in the observation state\n",
    "        if POSITION_PENALTY:\n",
    "            if abs(self.agent_bond_qty) > MAX_BOND_QTY:\n",
    "                print(\"Bond Quantity penalty reached\")\n",
    "                self.reward =- POSITION_PENALTY_AMOUNT\n",
    "        self.data = self.df.loc[self.tick,:]\n",
    "        self.future_prices = self.df.loc[self.tick + REWARD_HORIZON,:]\n",
    "        self.data_normalized = self.df_normalized.loc[self.tick,:]\n",
    "        self.state = self.data_normalized.tolist()\n",
    "        observation = np.array(self.state)\n",
    "        \n",
    "        return observation, self.reward, self.terminal, {} # 1. proximo book,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vk6XJzZNahYA",
    "outputId": "1a3138a6-b13f-412a-c428-c8d09ef45f6b"
   },
   "outputs": [],
   "source": [
    "# Environment checks\n",
    "\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "env = AY24BooksTrainEnv(df_train_FB_agent, daily_cuts)\n",
    "\n",
    "# This will check the custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model definitions and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTXrWBfyXQ5f",
    "outputId": "aa1723f2-acdd-4040-b464-df40f8b66acf"
   },
   "outputs": [],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import ACER\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize\n",
    "\n",
    "# Model selection\n",
    "\n",
    "MODEL = 'DQN' # Can be DQN, PPO2 or ACER\n",
    "\n",
    "# Logging path definition\n",
    "import datetime\n",
    "log_dir = \"logs\\\\\" + MODEL + \"_\" + \"EP\" + str(EPISODE_LENGTH) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Model creation\n",
    "\n",
    "# Case 1: DQN model\n",
    "if MODEL == 'DQN':\n",
    "    from stable_baselines.deepq.policies import MlpPolicy, LnMlpPolicy, CnnPolicy\n",
    "    env_TOB = AY24BooksTrainEnv(df_train_TOB_agent, daily_cuts)\n",
    "    env_FB  = AY24BooksTrainEnv(df_train_FB_agent, daily_cuts)\n",
    "    model_TOB = DQN(LnMlpPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed = 1, n_cpu_tf_sess = 1)\n",
    "    model_FB  = DQN(LnMlpPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed = 1, n_cpu_tf_sess = 1)\n",
    "\n",
    "# Case 2: PPO2 model\n",
    "elif MODEL == 'PPO2':\n",
    "    from stable_baselines.common.policies import MlpPolicy, CnnLstmPolicy, CnnPolicy, MlpLnLstmPolicy\n",
    "    env_TOB = DummyVecEnv([lambda: AY24BooksTrainEnv(df_train_TOB_agent, daily_cuts)])\n",
    "    env_FB  = DummyVecEnv([lambda: AY24BooksTrainEnv(df_train_FB_agent, daily_cuts)])\n",
    "    # Additional normalization layer\n",
    "    env_TOB = VecNormalize(env_TOB)\n",
    "    env_FB  = VecNormalize(env_FB)\n",
    "    model_TOB = PPO2(MlpPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed=1, n_cpu_tf_sess=1)\n",
    "    model_FB  = PPO2(MlpPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed=1, n_cpu_tf_sess=1)\n",
    "\n",
    "# Case 3: ACER model\n",
    "elif MODEL == 'ACER':\n",
    "    from stable_baselines.common.policies import MlpPolicy, CnnLstmPolicy, CnnPolicy, MlpLnLstmPolicy\n",
    "    env_TOB = DummyVecEnv([lambda: AY24BooksTrainEnv(df_train_TOB_agent, daily_cuts)])\n",
    "    env_FB  = DummyVecEnv([lambda: AY24BooksTrainEnv(df_train_FB_agent, daily_cuts)])\n",
    "    # Additional normalization layer\n",
    "    env_TOB = VecNormalize(env_TOB)\n",
    "    env_FB  = VecNormalize(env_FB)\n",
    "    model_TOB = ACER(MlpLnLstmPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed=1, n_cpu_tf_sess=1)\n",
    "    model_FB  = ACER(MlpLnLstmPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed=1, n_cpu_tf_sess=1)\n",
    "    \n",
    "else:\n",
    "    print('Invalid model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW OPEN TENSORBOARD\n",
    "# In Anaconda shell, change to env: \"conda activate tf-1.15\".\n",
    "# Type \"tensorboard --logdir logs\"\n",
    "# Open Tensorboard on browser at: http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agents Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING ENVIRONMENT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AY24BooksTestEnv(AY24BooksTrainEnv):\n",
    "    def reset(self):\n",
    "        self.tick = 0 # We set the testing data always at the beginning of each DF\n",
    "        self.agent_cash = 0\n",
    "        self.agent_bond_qty = 0\n",
    "        self.agent_bond_qty_graph = []\n",
    "        self.agent_cash_graph = []\n",
    "        self.agent_ptf_value = []  \n",
    "        self.data = self.df.loc[self.tick,:]\n",
    "        self.data_normalized = self.df_normalized.loc[self.tick,:]\n",
    "        self.state = self.data_normalized.tolist()\n",
    "        observation = np.array(self.state)\n",
    "        self.pending_buy  = []\n",
    "        self.pending_sell = []\n",
    "        return observation\n",
    "        \n",
    "    def step(self, action):\n",
    "        ptf_value = self.agent_bond_qty * self.data.Bid1_Price + self.agent_cash if self.agent_bond_qty > 0 \\\n",
    "                    else self.agent_bond_qty * self.data.Ask1_Price + self.agent_cash\n",
    "        # We check if the end of the dataset has been reached\n",
    "        self.terminal = self.tick > len(self.df.index.unique()) - REWARD_HORIZON - 1\n",
    "        if self.terminal:\n",
    "            print('Closing position in tick ', self.tick)\n",
    "            for i in self.pending_buy:\n",
    "                self.tick = i\n",
    "                self.data = self.df.loc[self.tick,:]\n",
    "                self.buy_bond()\n",
    "            self.pending_buy = []\n",
    "            for i in self.pending_sell:\n",
    "                self.tick = i\n",
    "                self.data = self.df.loc[self.tick,:]\n",
    "                self.sell_bond()\n",
    "            self.pending_sell = []\n",
    "            \n",
    "            print('-----END OF DATAFRAME-----')\n",
    "            print('Tick number is:', self.tick)\n",
    "            print('self.agent_cash=', round(self.agent_cash,2))\n",
    "            print('self.agent_bond_qty=', self.agent_bond_qty)\n",
    "            print('reward=',self.reward)\n",
    "            info = {'bid1_price': self.data.Bid1_Price,\n",
    "                    'agent_bond_qty': self.agent_bond_qty,\n",
    "                    'agent_cash': self.agent_cash,\n",
    "                    'agent_ptf_value': ptf_value}\n",
    "            return self.state, self.reward, self.terminal, info\n",
    "\n",
    "        if action == self.BUY:\n",
    "            print('Action: Buy')\n",
    "            self.buy_bond()\n",
    "            self.pending_sell.append(self.tick + REWARD_HORIZON)\n",
    "            print('I am buying at tick', self.tick, ' and will sell at tick', self.tick + REWARD_HORIZON)\n",
    "            \n",
    "        elif action == self.SELL:\n",
    "            print('Action: Sell')\n",
    "            self.sell_bond()\n",
    "            self.pending_buy.append(self.tick + REWARD_HORIZON)\n",
    "            print('I am selling at tick', self.tick, ' and will buy at tick', self.tick + REWARD_HORIZON)\n",
    "            \n",
    "        elif action == self.HOLD:\n",
    "            print('Action: Hold')\n",
    "            pass\n",
    "        else:\n",
    "            print('Error: Invalid action')\n",
    "        \n",
    "        # We close previously opened position\n",
    "        if self.tick in self.pending_buy:\n",
    "            print('Buying to close position...')\n",
    "            self.buy_bond()\n",
    "            self.pending_buy.remove(self.tick)\n",
    "        elif self.tick in self.pending_sell:\n",
    "            print('Selling to close position...')\n",
    "            self.sell_bond()\n",
    "            self.pending_sell.remove(self.tick)\n",
    "        \n",
    "        self.tick += 1\n",
    "\n",
    "        # Devolvemos en observation la info del proximo tick\n",
    "        self.data = self.df.loc[self.tick,:]\n",
    "        self.data_normalized = self.df_normalized.loc[self.tick,:]\n",
    "        self.cum_reward += self.reward\n",
    "        self.state = self.data_normalized.tolist()\n",
    "        observation = np.array(self.state)\n",
    "        ptf_value = self.agent_bond_qty * self.data.Bid1_Price + self.agent_cash if self.agent_bond_qty > 0 \\\n",
    "                    else self.agent_bond_qty * self.data.Ask1_Price + self.agent_cash\n",
    "        self.reward = 0 # We are only interested in the reward at end of episode at testing time\n",
    "        info = {'bid1_price': self.data.Bid1_Price,\n",
    "                'agent_bond_qty': self.agent_bond_qty,\n",
    "                'agent_cash': self.agent_cash,\n",
    "                'agent_ptf_value': ptf_value}\n",
    "        return observation, self.reward, self.terminal, info   # 1. proximo book,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 20 # Number of models that will predict on the test data, each more trained than the previous one\n",
    "INCREMENTAL_TIMESTEPS = 20000 # Ticks that each model will train (cumulative)\n",
    "TRAINING_TIMESTEPS = np.arange(0,(TRAINING_ITERATIONS+1) * INCREMENTAL_TIMESTEPS, INCREMENTAL_TIMESTEPS).tolist()\n",
    "RESET_MODEL = True\n",
    "SEED = 1 # Seed setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predictions on the training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and prediction loop on the training data, and information storage for future graphs\n",
    "\n",
    "agent_cash_TOB_iterations = []\n",
    "agent_cash_FB_iterations  = []\n",
    "\n",
    "for iteration in range(len(TRAINING_TIMESTEPS)):\n",
    "    print ('Training iteration number ', iteration)\n",
    "    agent_cash_TOB = []\n",
    "    agent_cash_FB  = []\n",
    "    \n",
    "    # Prediction loop\n",
    "    for i, date in enumerate(DATA_DATES[0:4]):\n",
    "        print('Day to predict is ', date)\n",
    "        # We need to vectorize the test environment for PPO2 or ACER models\n",
    "        if MODEL == 'PPO2' or 'ACER':\n",
    "            env_validation_TOB = DummyVecEnv([lambda: AY24BooksTestEnv(df_TOB[i])])\n",
    "            env_validation_FB  = DummyVecEnv([lambda: AY24BooksTestEnv(df_FB[i])])\n",
    "        else:\n",
    "            env_validation_TOB = AY24BooksTestEnv(df_TOB[i])\n",
    "            env_validation_FB  = AY24BooksTestEnv(df_FB[i])\n",
    "\n",
    "        obs_validation_TOB = env_validation_TOB.reset()\n",
    "        obs_validation_FB  = env_validation_FB.reset()\n",
    "        \n",
    "        # Prediction of the TOB agent\n",
    "        while(True):\n",
    "            action, _states = model_TOB.predict(obs_validation_TOB)\n",
    "            obs_validation_TOB, rewards, dones, info = env_validation_TOB.step(action)\n",
    "            if dones:\n",
    "                print('agent_cash_TOB_day is', info[0]['agent_cash'])\n",
    "                agent_cash_TOB_day = info[0]['agent_cash']\n",
    "                break\n",
    "\n",
    "        # Prediction of the FB agent\n",
    "        while(True):\n",
    "            action, _states = model_FB.predict(obs_validation_FB)\n",
    "            obs_validation_FB, rewards, dones, info = env_validation_FB.step(action)\n",
    "            if dones:\n",
    "                print('agent_cash_FB_day is', info[0]['agent_cash'])\n",
    "                agent_cash_FB_day = info[0]['agent_cash']\n",
    "                break\n",
    "        \n",
    "        agent_cash_TOB.append(agent_cash_TOB_day)\n",
    "        agent_cash_FB.append(agent_cash_FB_day)\n",
    "    \n",
    "    agent_cash_TOB_iterations.append(agent_cash_TOB)\n",
    "    agent_cash_FB_iterations.append(agent_cash_FB)\n",
    "    \n",
    "    if iteration < len(TRAINING_TIMESTEPS)-1:\n",
    "        # We train the model now and run the prediction in the next iteration\n",
    "        if RESET_MODEL is True:\n",
    "            del model_TOB\n",
    "            del model_FB\n",
    "            if MODEL == 'DQN':\n",
    "                model_TOB = DQN(LnMlpPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed = SEED, n_cpu_tf_sess = 1)\n",
    "                model_FB  = DQN(LnMlpPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed = SEED, n_cpu_tf_sess = 1)\n",
    "            elif MODEL == 'PPO2':\n",
    "                model_TOB = PPO2(MlpPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed=SEED, n_cpu_tf_sess=1)\n",
    "                model_FB  = PPO2(MlpPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed=SEED, n_cpu_tf_sess=1)\n",
    "            elif MODEL == 'ACER':\n",
    "                model_TOB = ACER(MlpLnLstmPolicy, env_TOB, verbose=1, tensorboard_log=\".\\\\{}_TOB\".format(log_dir), seed=SEED, n_cpu_tf_sess=1)\n",
    "                model_FB  = ACER(MlpLnLstmPolicy, env_FB, verbose=1, tensorboard_log=\".\\\\{}_FB\".format(log_dir), seed=SEED, n_cpu_tf_sess=1)\n",
    "            print('I will train for this amount of timesteps:', TRAINING_TIMESTEPS[iteration+1])\n",
    "            model_TOB.learn(total_timesteps=TRAINING_TIMESTEPS[iteration+1])\n",
    "            model_FB.learn(total_timesteps=TRAINING_TIMESTEPS[iteration+1])\n",
    "        else:\n",
    "            print('I will train for this amount of timesteps:', TRAINING_TIMESTEPS[iteration+1]-TRAINING_TIMESTEPS[iteration])\n",
    "            model_TOB.learn(total_timesteps=TRAINING_TIMESTEPS[iteration+1]-TRAINING_TIMESTEPS[iteration])\n",
    "            model_FB.learn(total_timesteps=TRAINING_TIMESTEPS[iteration+1]-TRAINING_TIMESTEPS[iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_profit() function for evaluating the performance of the TOB and FB agents\n",
    "# predictions on the training and validation sets dates\n",
    "\n",
    "def print_profit(TOB_series, FB_series):\n",
    "    df_TOB = pd.DataFrame(TOB_series)\n",
    "    df_TOB.columns = ['TRAIN DAY %d %s TOB'% (i, item) for i,item in enumerate(TRAIN_DATA)] + ['VALIDATION DAY %s TOB' % VALIDATION_DATA]\n",
    "    df_FB = pd.DataFrame(FB_series)\n",
    "    df_FB.columns  = ['TRAIN DAY %d %s FB'% (i, item) for i,item in enumerate(TRAIN_DATA)] + ['VALIDATION DAY %s FB' % VALIDATION_DATA]\n",
    "    df = pd.concat([df_TOB, df_FB], axis = 1)\n",
    "    ax = df.plot(figsize = (16,8),\n",
    "            title=('Agent Profit' + ' (' + MODEL + ', ENT=' + str(ENTRIES_RETAINED) + ', RH=' + str(REWARD_HORIZON) + ')'),\n",
    "            xlabel = ('Training Timesteps'),\n",
    "            rot = 45,\n",
    "            ylabel = ('Agent Profit ($)'),\n",
    "            sort_columns = True,\n",
    "            grid = True,\n",
    "            style = ['-','-','-','-','--','--','--','--'],\n",
    "            color = ['b', 'g', 'r', 'c', 'b', 'g', 'r', 'c'],\n",
    "            linewidth = 1,\n",
    "            )\n",
    "    ax.set_xticks(df.index)\n",
    "    ax.set_xticklabels(TRAINING_TIMESTEPS, rotation=45)\n",
    "    \n",
    "    df_validation = df[['VALIDATION DAY %s TOB' % VALIDATION_DATA, 'VALIDATION DAY %s FB' % VALIDATION_DATA]]\n",
    "    ax_validation = df_validation.plot(figsize = (16,8),\n",
    "            title=('Agent Profit' + ' (' + MODEL + ', ENT=' + str(ENTRIES_RETAINED) + ', RH=' + str(REWARD_HORIZON) + ')'),\n",
    "            xlabel = ('Training Timesteps'),\n",
    "            rot = 45,\n",
    "            ylabel = ('Agent Profit ($)'),\n",
    "            sort_columns = True,\n",
    "            grid = True,\n",
    "            color = ['r', 'g'],#, 'r', 'c', 'b', 'g', 'r', 'c'],\n",
    "            linewidth = 1,\n",
    "            )\n",
    "    ax_validation.set_xticks(df_validation.index)\n",
    "    ax_validation.set_xticklabels(TRAINING_TIMESTEPS, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce graphs of training vs validation profit and isolated validation profit\n",
    "print_profit(agent_cash_TOB_iterations, agent_cash_FB_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on the testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We test now on the test day\n",
    "\n",
    "# We need to vectorize the test environment for PPO2 or ACER models\n",
    "if MODEL == 'PPO2' or 'ACER':\n",
    "    env_test_TOB = DummyVecEnv([lambda: AY24BooksTestEnv(df_test_TOB_agent)])\n",
    "    env_test_FB  = DummyVecEnv([lambda: AY24BooksTestEnv(df_test_FB_agent)])\n",
    "else:\n",
    "    env_test_TOB = AY24BooksTestEnv(df_test_TOB_agent)\n",
    "    env_test_FB  = AY24BooksTestEnv(df_test_FB_agent)\n",
    "\n",
    "obs_test_TOB = env_test_TOB.reset()\n",
    "obs_test_FB  = env_test_FB.reset()\n",
    "\n",
    "bid1_price_day = []\n",
    "agent_bond_qty_TOB_day = []\n",
    "agent_bond_qty_FB_day  = []\n",
    "agent_cash_TOB_day = []\n",
    "agent_cash_FB_day  = []\n",
    "agent_ptf_value_TOB_day = []\n",
    "agent_ptf_value_FB_day  = []\n",
    "\n",
    "# Prediction of the TOB agent\n",
    "while(True):\n",
    "    action, _states = model_TOB.predict(obs_test_TOB)\n",
    "    obs_test_TOB, rewards, dones, info = env_test_TOB.step(action)\n",
    "    bid1_price_day.append(info[0]['bid1_price'])\n",
    "    agent_bond_qty_TOB_day.append(info[0]['agent_bond_qty'])\n",
    "    agent_cash_TOB_day.append(info[0]['agent_cash'])\n",
    "    agent_ptf_value_TOB_day.append(info[0]['agent_ptf_value'])\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "# Prediction of the FB agent\n",
    "while(True):\n",
    "    action, _states = model_FB.predict(obs_test_FB)\n",
    "    obs_test_FB, rewards, dones, info = env_test_FB.step(action)\n",
    "    agent_bond_qty_FB_day.append(info[0]['agent_bond_qty'])\n",
    "    agent_cash_FB_day.append(info[0]['agent_cash'])\n",
    "    agent_ptf_value_FB_day.append(info[0]['agent_ptf_value'])\n",
    "    if dones:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiton of another printing function to evaluate the bond positions and ptf value of the test day predictions\n",
    "def print_graphs(benchmark_series, TOB_series, FB_series, title, ylabel, scaled = False):\n",
    "    if scaled:\n",
    "        print('Bond QTY final TOB: ', TOB_series[-1])\n",
    "        print('Bond QTY final FB: ', FB_series[-1])\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title(title + ' (' + MODEL + ', EP=' + str(EPISODE_LENGTH) + ', RH=' + str(REWARD_HORIZON) + ')')\n",
    "        #plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Bid1_Price($)')\n",
    "        plt.plot(benchmark_series, label='Bid1_Price', alpha=1, color = 'blue')\n",
    "        plt.subplot(2, 1, 2)\n",
    "        #plt.title('Bond Quantity')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.plot(TOB_series, label=\"TOB\", color = 'red')\n",
    "        plt.plot(FB_series, label=\"FB\", color = 'green')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    else:\n",
    "        print('Cash final TOB: $', TOB_series[-1])\n",
    "        print('Cash final FB: $', FB_series[-1])\n",
    "        benchmark_series = (benchmark_series - benchmark_series[0])\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.title(title + ' (' + MODEL + ', EP=' + str(EPISODE_LENGTH) + ', RH=' + str(REWARD_HORIZON) + ')')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.plot(benchmark_series, label='Long 1 Bond', alpha=0.5, color = 'blue')\n",
    "        plt.plot(TOB_series, label=\"TOB\", color = 'red')\n",
    "        plt.plot(FB_series, label=\"FB\", color = 'green')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce test day graphs\n",
    "date = TEST_DATA\n",
    "print_graphs(bid1_price_day, agent_bond_qty_TOB_day, agent_bond_qty_FB_day, 'TEST ' + date + ' Agent Bond Quantity', 'Bond Quantity', scaled=True)\n",
    "print_graphs(bid1_price_day, agent_ptf_value_TOB_day, agent_ptf_value_FB_day, 'TEST ' + date + ' Agent Portfolio Value', 'PTF Value ($)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final graph to plot once all the seeds configuration values have been run.\n",
    "# Values in d are the results of different previous experiments.\n",
    "d = {'TEST DAY 07-01-20 TOB': [-4.13, -1.67, -3.21, -5.18, -3.79, -5.14, -2.71, -2.28], 'TEST DAY 07-01-20 FB': [-3.36, -6.58, -3.21, -3.56, -2.63, -2.91, -3.77, -3.18]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.index = [1,2,3,4,5,6,7,8]\n",
    "ax = df.plot(figsize = (16,8),\n",
    "            title=('Agent Profit on Test Data' + ' (' + MODEL + ', ENT=' + str(ENTRIES_RETAINED) + ', RH=' + str(REWARD_HORIZON) + ')'),\n",
    "            xlabel = ('DQN Model #Seed'),\n",
    "            rot = 0,\n",
    "            ylabel = ('Agent Profit ($)'),\n",
    "            sort_columns = True,\n",
    "            grid = True,\n",
    "            style = ['-','-'],\n",
    "            color = ['r', 'g'],\n",
    "            linewidth = 1,\n",
    "            )\n",
    "ax.set_xticks(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rl_ay24.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.15]",
   "language": "python",
   "name": "conda-env-tf-1.15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
