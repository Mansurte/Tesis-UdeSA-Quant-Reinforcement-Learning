{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alonso, Ariel - MFE Thesis - FIX Books processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplefix as sf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FIX FIELDS CONSTANTS '''\n",
    "\n",
    "REFRESH_TYPE = '35'\n",
    "FULL_REFRESH = b'W'\n",
    "INCREMENTAL_REFRESH = b'X'\n",
    "\n",
    "TICKER = '55'\n",
    "AY24D = b'AY24D'\n",
    "AY24C = b'AY24C'\n",
    "AY24 = b'AY24'\n",
    "\n",
    "PRICE = '270'\n",
    "SIZE = '271'\n",
    "\n",
    "NUM_MESSAGES = '268'\n",
    "\n",
    "MESSAGE_TYPE = '269'\n",
    "BID = b'0'\n",
    "OFFER = b'1'\n",
    "\n",
    "ACTION = '279'\n",
    "NEW = b'0'\n",
    "CHANGE = b'1'\n",
    "DELETE = b'2'\n",
    "\n",
    "POSITION = '290'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path AND specific file to process\n",
    "DATA_PATH = 'C:/Users/Ariel/Documents/Maestria/TESIS/FIX/Logs FIX/MD-2019-12-18-price-depth-T2.log'\n",
    "\n",
    "BOOK_COLUMNS = ['Bid1',\n",
    "                'Bid2',\n",
    "                'Bid3',\n",
    "                'Bid4',\n",
    "                'Bid5',\n",
    "                'Ask1',\n",
    "                'Ask2',\n",
    "                'Ask3',\n",
    "                'Ask4',\n",
    "                'Ask5',\n",
    "                ]\n",
    "ID_COLUMNS = ['Message_type', 'Ticker']\n",
    "DF_COLUMNS = ID_COLUMNS + BOOK_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(DATA_PATH,\n",
    "                     names=['date', 'time', 'raw_data'],\n",
    "                     delimiter=' ')\n",
    "\n",
    "raw_df['Datetime'] = pd.to_datetime(raw_df['date'] + ' ' + raw_df['time'])\n",
    "raw_df = raw_df.drop(['date', 'time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX Messages Parsing & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for processing the FIX messages\n",
    "\n",
    "def refresh_type(message):\n",
    "    ''' Returns the type of market data refresh type Fix message'''\n",
    "    if message.get(REFRESH_TYPE) == FULL_REFRESH:\n",
    "        return 'FULL'\n",
    "    elif message.get(REFRESH_TYPE) == INCREMENTAL_REFRESH:\n",
    "        return 'INCREMENTAL'\n",
    "    else:\n",
    "        print('Unknown refresh message type')\n",
    "\n",
    "\n",
    "def get_message_from_df(df):\n",
    "    ''' Returns a Fix Message object from the df raw data'''\n",
    "    parser = sf.parser.FixParser()\n",
    "    parser.append_buffer(df.raw_data)\n",
    "    return parser.get_message()\n",
    "\n",
    "\n",
    "def parse_full_refresh_message(message):\n",
    "    '''\n",
    "    Parses full refresh market data messages and returns a dict\n",
    "    with actual order book data\n",
    "    '''\n",
    "    data = [message.get(REFRESH_TYPE).decode('utf-8'),\n",
    "            message.get(TICKER).decode('utf-8')]\n",
    "    empty_book_entries = 0\n",
    "    for num_entry in range(1, 6):\n",
    "        if message.get(MESSAGE_TYPE, num_entry) == BID:\n",
    "            data.append((float(message.get(PRICE, num_entry)),\n",
    "                         float(message.get(SIZE, num_entry))))\n",
    "        else:\n",
    "            empty_book_entries += 1\n",
    "            data.append((0,0)) # empty bids in the book\n",
    "    for num_entry in range(6, 11):\n",
    "        if message.get(MESSAGE_TYPE, num_entry - empty_book_entries) == OFFER:\n",
    "            data.append((float(message.get(PRICE, num_entry - empty_book_entries)),\n",
    "                         float(message.get(SIZE, num_entry - empty_book_entries))))\n",
    "        else:\n",
    "            data.append((0,0)) # empty offers in the book\n",
    "    return dict(zip(DF_COLUMNS, data))\n",
    "\n",
    "\n",
    "def parse_incremental_refresh_message(message):\n",
    "    '''\n",
    "    Parses incremental refresh market data messages and returns a dict\n",
    "    containing info about the incremental message information\n",
    "    '''\n",
    "    data_dict = {}\n",
    "    data_dict['Message_type'] = message.get(REFRESH_TYPE).decode('utf-8')\n",
    "    data_dict['Ticker'] = message.get(TICKER).decode('utf-8')\n",
    "    for num_updates in range(1, int(message.get(NUM_MESSAGES)) + 1):\n",
    "        if message.get(MESSAGE_TYPE, num_updates) == BID:\n",
    "            if message.get(ACTION, num_updates) == NEW:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Bid',\n",
    "                                                             'New',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "            elif message.get(ACTION, num_updates) == CHANGE:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Bid',\n",
    "                                                             'Change',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "            elif message.get(ACTION, num_updates) == DELETE:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Bid',\n",
    "                                                             'Delete',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "        elif message.get(MESSAGE_TYPE, num_updates) == OFFER:\n",
    "            if message.get(ACTION, num_updates) == NEW:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Ask',\n",
    "                                                             'New',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "            elif message.get(ACTION, num_updates) == CHANGE:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Ask',\n",
    "                                                             'Change',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "            elif message.get(ACTION, num_updates) == DELETE:\n",
    "                data_dict['Action{}'.format(num_updates)] = ('Ask',\n",
    "                                                             'Delete',\n",
    "                                                             float(message.get(PRICE, num_updates)),\n",
    "                                                             float(message.get(SIZE, num_updates)),\n",
    "                                                             int(message.get(POSITION, num_updates)),\n",
    "                                                             )\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def parse_fix_raw_data(df):\n",
    "    ''' Parses a generic Fix message and returns a dict'''\n",
    "    message = get_message_from_df(df)\n",
    "    if refresh_type(message) == 'FULL':  # Book full refresh\n",
    "        full_refresh_data = parse_full_refresh_message(message)\n",
    "        return full_refresh_data\n",
    "    elif refresh_type(message) == 'INCREMENTAL':  # Book incremental refresh\n",
    "        incremental_refresh_data = parse_incremental_refresh_message(message)\n",
    "        return incremental_refresh_data\n",
    "    else:\n",
    "        print('Unknown message type')\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We now pass the dataframe to the parsing method\n",
    "aux_df = raw_df.apply(parse_fix_raw_data, axis=1, result_type='expand')\n",
    "\n",
    "#  Here we combine both dataframes to keep all data together in one df\n",
    "df_data = pd.concat([raw_df, aux_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we need to split the dataframe in 3 by subsetting: AY24D, AY24C and AY24\n",
    "df_AY24D = df_data[df_data['Ticker'] == 'AY24D']\n",
    "df_AY24C = df_data[df_data['Ticker'] == 'AY24C']\n",
    "df_AY24 = df_data[df_data['Ticker'] == 'AY24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we have the dataframes with the full refresh fields but\n",
    "# the incremental messages are not yet incorporated in the book\n",
    "\n",
    "# First we remove all the incremental messages that don't modify\n",
    "# the order book and reset the indexes\n",
    "df_AY24D = df_AY24D.dropna(thresh=5)\n",
    "df_AY24D.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_AY24C = df_AY24C.dropna(thresh=5)\n",
    "df_AY24C.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_AY24 = df_AY24.dropna(thresh=5)\n",
    "df_AY24.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to parse the incremental messages and fill the order book\n",
    "# for all the rows\n",
    "\n",
    "\n",
    "def new_order_book(row, last_row):\n",
    "    row[BOOK_COLUMNS] = last_row\n",
    "    if row['Action1'][1] == 'Change':\n",
    "        position = row['Action1'][0] + str(row['Action1'][4])\n",
    "        row[position] = (row['Action1'][2], row['Action1'][3])\n",
    "    elif row['Action1'][1] == 'Delete':\n",
    "        for pos in range(row['Action1'][4], 5):\n",
    "            aux = pos + 1\n",
    "            position_original = row['Action1'][0] + str(aux)\n",
    "            position_destination = row['Action1'][0] + str(pos)\n",
    "            row[position_destination] = row[position_original]\n",
    "        aux2 = row['Action1'][0] + '5'\n",
    "        if not pd.isnull(row['Action2']):\n",
    "            row[aux2] = (row['Action2'][2], row['Action2'][3])\n",
    "    elif row['Action1'][1] == 'New':\n",
    "        for pos in range(5, row['Action1'][4], -1):\n",
    "            aux = pos - 1\n",
    "            position_original = row['Action1'][0] + str(aux)\n",
    "            position_destination = row['Action1'][0] + str(pos)\n",
    "            row[position_destination] = row[position_original]\n",
    "        aux2 = row['Action1'][0] + str(row['Action1'][4])\n",
    "        row[aux2] = (row['Action1'][2], row['Action1'][3])\n",
    "    return row[BOOK_COLUMNS]\n",
    "\n",
    "\n",
    "def process_incremental_messages(df):\n",
    "    ''' Incorporates the incremental messages info to the df order book'''\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Message_type'] == 'X':\n",
    "            last_row_data = df.loc[index - 1, BOOK_COLUMNS]\n",
    "            new_data = new_order_book(row, last_row_data)\n",
    "            df.at[index, BOOK_COLUMNS] = new_data.values\n",
    "\n",
    "\n",
    "# Now we apply the processing of the incremental messages to the 3 dataframes\n",
    "process_incremental_messages(df_AY24D)\n",
    "process_incremental_messages(df_AY24C)\n",
    "process_incremental_messages(df_AY24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to modify these dataframes to separate the Bid and Ask tuples\n",
    "# We create one new column for price and size for each Bid and Ask\n",
    "\n",
    "\n",
    "def separate_bid_and_ask(df):\n",
    "    ''' Takes a dataframe with a tuple of price and size and returns\n",
    "    a dataframe with that data separated in two different columns'''\n",
    "    for column in BOOK_COLUMNS:\n",
    "        df[[column+'_Price', column+'_Size']] = pd.DataFrame(df[column].tolist(), index=df.index)\n",
    "    df.drop(BOOK_COLUMNS, axis=1)\n",
    "\n",
    "\n",
    "separate_bid_and_ask(df_AY24D)\n",
    "separate_bid_and_ask(df_AY24C)\n",
    "separate_bid_and_ask(df_AY24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set datetime column as index. df must have a valid 'Datetime' column'''\n",
    "df_AY24D = df_AY24D.set_index('Datetime',drop=False)\n",
    "df_AY24C = df_AY24C.set_index('Datetime',drop=False)\n",
    "df_AY24 = df_AY24.set_index('Datetime',drop=False)\n",
    "\n",
    "\n",
    "# We now remove the duplicate indexes keeping only the last state (this caused problems)\n",
    "df_AY24D = df_AY24D[~df_AY24D.index.duplicated(keep='last')]\n",
    "df_AY24C = df_AY24C[~df_AY24C.index.duplicated(keep='last')]\n",
    "df_AY24 = df_AY24[~df_AY24.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book saving as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of columns to save in the .csv files\n",
    "USEFUL_COLUMNS = ['Bid1_Price',\n",
    "                'Bid2_Price',\n",
    "                'Bid3_Price',\n",
    "                'Bid4_Price',\n",
    "                'Bid5_Price',\n",
    "                'Ask1_Price',\n",
    "                'Ask2_Price',\n",
    "                'Ask3_Price',\n",
    "                'Ask4_Price',\n",
    "                'Ask5_Price',\n",
    "                'Bid1_Size',\n",
    "                'Bid2_Size',\n",
    "                'Bid3_Size',\n",
    "                'Bid4_Size',\n",
    "                'Bid5_Size',\n",
    "                'Ask1_Size',\n",
    "                'Ask2_Size',\n",
    "                'Ask3_Size',\n",
    "                'Ask4_Size',\n",
    "                'Ask5_Size',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual saving of the files\n",
    "df_AY24D[USEFUL_COLUMNS].to_csv('C:/Users/Ariel/Documents/Maestria/TESIS/Mi Tesis/AY24D_18-12-19.csv')\n",
    "df_AY24C[USEFUL_COLUMNS].to_csv('C:/Users/Ariel/Documents/Maestria/TESIS/Mi Tesis/AY24C_18-12-19.csv')\n",
    "df_AY24[USEFUL_COLUMNS].to_csv('C:/Users/Ariel/Documents/Maestria/TESIS/Mi Tesis/AY24_18-12-19.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:simplefix]",
   "language": "python",
   "name": "conda-env-simplefix-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
